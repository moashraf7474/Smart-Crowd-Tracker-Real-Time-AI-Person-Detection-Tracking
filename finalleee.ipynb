{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moashraf7474/Smart-Crowd-Tracker-Real-Time-AI-Person-Detection-Tracking/blob/main/finalleee.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eabb7c8b",
      "metadata": {
        "id": "eabb7c8b",
        "outputId": "9673b2e8-3a71-42c4-b59c-4aaf30292a06"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics supervision opencv-python-headless -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a32d8e45",
      "metadata": {
        "id": "a32d8e45",
        "outputId": "2bc66ad8-0ee9-4b48-fd31-48d562bb1b4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\pyproj\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "d:\\pyproj\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "d:\\pyproj\\.venv\\Lib\\site-packages\\deep_sort_realtime\\embedder\\embedder_pytorch.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n",
            "d:\\pyproj\\.venv\\Lib\\site-packages\\deep_sort_realtime\\embedder\\embedder_pytorch.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.model.load_state_dict(torch.load(model_wts_path))\n"
          ]
        }
      ],
      "source": [
        "#rcnn upload\n",
        "# Optimized: Faster R-CNN (MobileNet) + DeepSORT + Tkinter (Video upload)\n",
        "# - Uses smaller detector: fasterrcnn_mobilenet_v3_large_fpn\n",
        "# - Uses GPU + mixed precision if available\n",
        "# - Resizes frames before inference (scales boxes back)\n",
        "# - Frame skipping option to improve throughput\n",
        "# - Better filtering of tracks for stable counting\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import threading\n",
        "import tkinter as tk\n",
        "from tkinter import ttk, filedialog\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "import os\n",
        "from datetime import datetime\n",
        "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn\n",
        "from torchvision.ops import boxes as box_ops\n",
        "\n",
        "# -----------------------\n",
        "# Config / Tunables\n",
        "# -----------------------\n",
        "TARGET_WIDTH = 640            # resize width for inference (lower -> faster)\n",
        "FRAME_SKIP = 1                # process every FRAME_SKIP-th frame (1 = every frame)\n",
        "CONF_THRESH = 0.6             # detection score threshold (higher reduces false positives)\n",
        "TRACK_TIME_SINCE_UPDATE = 2   # allow tracks with time_since_update <= this to count\n",
        "MODEL_TO_USE = \"mobilenet\"    # label only; code uses mobilenet-based model\n",
        "\n",
        "# -----------------------\n",
        "# Device & Model\n",
        "# -----------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Smaller, faster detector\n",
        "model = fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# -----------------------\n",
        "# Tracker\n",
        "# -----------------------\n",
        "# Keep default DeepSort params but you can tune max_age/n_init/nn_budget\n",
        "tracker = DeepSort(max_age=30, n_init=3, nn_budget=70)\n",
        "\n",
        "# -----------------------\n",
        "# Globals\n",
        "# -----------------------\n",
        "running = False\n",
        "recording = False\n",
        "out = None\n",
        "video_save_path = None\n",
        "video_path = None\n",
        "\n",
        "download_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
        "os.makedirs(download_path, exist_ok=True)\n",
        "\n",
        "GREEN = (0, 255, 0)\n",
        "RED = (0, 0, 255)\n",
        "YELLOW = (0, 255, 255)\n",
        "PERSON_CLASS_ID = 1  # COCO person\n",
        "\n",
        "# -----------------------\n",
        "# Helpers\n",
        "# -----------------------\n",
        "def resize_and_scale(frame, target_w=TARGET_WIDTH):\n",
        "    h, w = frame.shape[:2]\n",
        "    if w <= target_w:\n",
        "        return frame, 1.0  # no scaling\n",
        "    scale = target_w / float(w)\n",
        "    new_h = int(h * scale)\n",
        "    resized = cv2.resize(frame, (target_w, new_h))\n",
        "    return resized, scale\n",
        "\n",
        "def scale_boxes(boxes, scale):\n",
        "    # boxes: Nx4 with x1,y1,x2,y2 on resized image -> scale up to original\n",
        "    if scale == 1.0:\n",
        "        return boxes\n",
        "    inv = 1.0 / scale\n",
        "    return boxes * inv\n",
        "\n",
        "# -----------------------\n",
        "# Main processing\n",
        "# -----------------------\n",
        "def detect_and_track_video():\n",
        "    global running, recording, out, video_save_path, video_path, tracker\n",
        "    if not video_path:\n",
        "        print(\"‚ö† Please upload a video first.\")\n",
        "        return\n",
        "\n",
        "    # reset tracker state for new video\n",
        "    try:\n",
        "        tracker.tracker.clear_tracks()\n",
        "    except Exception:\n",
        "        # safe fallback if internals differ\n",
        "        tracker = DeepSort(max_age=30, n_init=3, nn_budget=70)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\" Cannot open video.\")\n",
        "        return\n",
        "\n",
        "    frame_idx = 0\n",
        "\n",
        "    # Use autocast for mixed precision on CUDA\n",
        "    use_amp = (device.type == \"cuda\")\n",
        "    if use_amp:\n",
        "        scaler_ctx = torch.cuda.amp.autocast\n",
        "    else:\n",
        "        # dummy context manager when no amp available\n",
        "        from contextlib import nullcontext\n",
        "        scaler_ctx = nullcontext\n",
        "\n",
        "    while running:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_idx += 1\n",
        "        # Optionally skip frames to boost throughput\n",
        "        if FRAME_SKIP > 1 and (frame_idx % FRAME_SKIP) != 0:\n",
        "            # still show frame but don't run detection (optional: could skip showing as well)\n",
        "            cv2.imshow(\"Tracking (skipping frames for speed)\", frame)\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "            continue\n",
        "\n",
        "        # Resize for faster inference, keep scale to map boxes back\n",
        "        resized, scale = resize_and_scale(frame, TARGET_WIDTH)\n",
        "        img_rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n",
        "        img_tensor = torch.from_numpy(img_rgb / 255.0).permute(2, 0, 1).float().to(device).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with scaler_ctx():\n",
        "                preds = model(img_tensor)[0]\n",
        "\n",
        "        boxes = preds['boxes'].cpu().numpy()    # x1,y1,x2,y2 on resized frame\n",
        "        scores = preds['scores'].cpu().numpy()\n",
        "        labels = preds['labels'].cpu().numpy()\n",
        "\n",
        "        # Filter detections: score + person class\n",
        "        detections = []\n",
        "        for box, score, label in zip(boxes, scores, labels):\n",
        "            if score < CONF_THRESH:\n",
        "                continue\n",
        "            if int(label) != PERSON_CLASS_ID:\n",
        "                continue\n",
        "            # scale box coords back to original frame size\n",
        "            x1, y1, x2, y2 = box\n",
        "            x1, y1, x2, y2 = [int(x / (scale if scale != 0 else 1.0)) for x in (x1, y1, x2, y2)]\n",
        "            w = x2 - x1\n",
        "            h = y2 - y1\n",
        "            # deep_sort_realtime expects [x, y, w, h]\n",
        "            detections.append(([float(x1), float(y1), float(w), float(h)], float(score), \"person\"))\n",
        "\n",
        "        # Update tracker with detections on original-sized frame\n",
        "        tracks = tracker.update_tracks(detections, frame=frame)\n",
        "\n",
        "        # Count only stable confirmed tracks\n",
        "        current_count = 0\n",
        "        for track in tracks:\n",
        "            # require confirmed and not too stale\n",
        "            if not track.is_confirmed():\n",
        "                continue\n",
        "            if track.time_since_update > TRACK_TIME_SINCE_UPDATE:\n",
        "                continue\n",
        "            current_count += 1\n",
        "            x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), GREEN, 2)\n",
        "            cv2.putText(frame, f\"ID:{track.track_id}\", (x1, max(y1 - 8, 8)),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, GREEN, 2)\n",
        "\n",
        "        # Display count on frame\n",
        "        cv2.putText(frame, f\"Current: {current_count}\", (20, 40),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, RED, 2)\n",
        "\n",
        "        # Recording\n",
        "        if recording:\n",
        "            if out is None:\n",
        "                filename = f\"tracked_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4\"\n",
        "                video_save_path = os.path.join(download_path, filename)\n",
        "                fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "                h, w = frame.shape[:2]\n",
        "                out = cv2.VideoWriter(video_save_path, fourcc, 20.0, (w, h))\n",
        "                print(\"üé• Recording ->\", video_save_path)\n",
        "            out.write(frame)\n",
        "\n",
        "        cv2.imshow(\"Tracking (optimized)\", frame)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    if out is not None:\n",
        "        out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# -----------------------\n",
        "# Basic controls + tkinter UI (same pattern)\n",
        "# -----------------------\n",
        "def start_tracking():\n",
        "    global running\n",
        "    if not running and video_path:\n",
        "        running = True\n",
        "        threading.Thread(target=detect_and_track_video, daemon=True).start()\n",
        "    elif not video_path:\n",
        "        print(\"‚ö† Upload a video before starting.\")\n",
        "\n",
        "def stop_tracking():\n",
        "    global running\n",
        "    running = False\n",
        "\n",
        "def start_recording():\n",
        "    global recording, out, video_save_path\n",
        "    if not recording:\n",
        "        recording = True\n",
        "        video_save_path = None\n",
        "        out = None\n",
        "        print(\"üé¨ Recording started\")\n",
        "\n",
        "def stop_recording():\n",
        "    global recording, out, video_save_path\n",
        "    if recording:\n",
        "        recording = False\n",
        "        if out is not None:\n",
        "            out.release()\n",
        "            out = None\n",
        "        if video_save_path:\n",
        "            print(\"üíæ Recording saved to:\", video_save_path)\n",
        "        else:\n",
        "            print(\"Recording stopped (no file)\")\n",
        "\n",
        "def upload_video():\n",
        "    global video_path, tracker\n",
        "    file_path = filedialog.askopenfilename(\n",
        "        title=\"Select a video file\",\n",
        "        filetypes=[(\"Video files\", \"*.mp4 *.avi *.mov *.mkv\")]\n",
        "    )\n",
        "    if file_path:\n",
        "        video_path = file_path\n",
        "        # reset tracker on new video to avoid leftover IDs\n",
        "        try:\n",
        "            tracker.tracker.clear_tracks()\n",
        "        except Exception:\n",
        "            tracker = DeepSort(max_age=30, n_init=3, nn_budget=70)\n",
        "        print(\"‚úÖ Video loaded:\", video_path)\n",
        "        video_label.config(text=f\"Video: {os.path.basename(video_path)}\", fg=\"lightgreen\")\n",
        "\n",
        "# Tkinter UI\n",
        "root = tk.Tk()\n",
        "root.title(\"Optimized Tracker\")\n",
        "root.geometry(\"460x420\")\n",
        "root.configure(bg=\"#202020\")\n",
        "\n",
        "style = ttk.Style()\n",
        "style.configure(\"TButton\", font=(\"Arial\", 12), padding=10)\n",
        "\n",
        "title_label = tk.Label(root, text=\"Optimized: Detector + DeepSORT\",\n",
        "                       bg=\"#202020\", fg=\"white\", font=(\"Arial\", 14))\n",
        "title_label.pack(pady=15)\n",
        "\n",
        "ttk.Button(root, text=\" Upload Video\", command=upload_video).pack(pady=5)\n",
        "video_label = tk.Label(root, text=\"No video selected\", bg=\"#202020\", fg=\"gray\", font=(\"Arial\", 10))\n",
        "video_label.pack()\n",
        "\n",
        "ttk.Button(root, text=\" Start Tracking\", command=start_tracking).pack(pady=5)\n",
        "ttk.Button(root, text=\" Stop Tracking\", command=stop_tracking).pack(pady=5)\n",
        "ttk.Button(root, text=\" Start Recording\", command=start_recording).pack(pady=5)\n",
        "ttk.Button(root, text=\" Stop Recording\", command=stop_recording).pack(pady=5)\n",
        "ttk.Button(root, text=\" Exit\", command=root.destroy).pack(pady=5)\n",
        "\n",
        "root.mainloop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e648b8c",
      "metadata": {
        "id": "3e648b8c",
        "outputId": "e47cd902-72e9-47be-8417-1a06cb05e29c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Using device: cpu\n",
            "‚è≥ Loading YOLOv8 model...\n",
            "‚úÖ Model loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================== edit\n",
        "# YOLOv8 + ByteTrack + Tkinter GUI (Webcam + Video Tracking)\n",
        "# - Detects and tracks people in webcam or uploaded video\n",
        "# - GPU accelerated (if available)\n",
        "# - Frame skipping, resize optimization, and recording\n",
        "# - Shows ONLY current people count (no unique counter)\n",
        "# ==============================================================\n",
        "\n",
        "import cv2\n",
        "import threading\n",
        "import torch\n",
        "import os\n",
        "import tkinter as tk\n",
        "from tkinter import ttk, filedialog\n",
        "from ultralytics import YOLO\n",
        "from datetime import datetime\n",
        "\n",
        "# -----------------------\n",
        "# Config\n",
        "# -----------------------\n",
        "MODEL_NAME = \"yolov8n.pt\"   # lightweight model\n",
        "TARGET_WIDTH = 640          # resize width (lower = faster)\n",
        "FRAME_SKIP = 1              # 1 = every frame, 2 = every other frame\n",
        "CONF_THRESH = 0.5           # YOLO confidence threshold\n",
        "PERSON_CLASS_ID = 0         # YOLO person ID\n",
        "\n",
        "# -----------------------\n",
        "# Globals\n",
        "# -----------------------\n",
        "running = False\n",
        "recording = False\n",
        "using_webcam = False\n",
        "out = None\n",
        "video_save_path = None\n",
        "video_path = None\n",
        "download_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
        "os.makedirs(download_path, exist_ok=True)\n",
        "\n",
        "GREEN = (0, 255, 0)\n",
        "RED = (0, 0, 255)\n",
        "\n",
        "# -----------------------\n",
        "# Device & Model\n",
        "# -----------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\" Using device:\", device)\n",
        "\n",
        "print(\" Loading YOLOv8 model...\")\n",
        "model = YOLO(MODEL_NAME)\n",
        "model.to(device)\n",
        "print(\" Model loaded successfully.\")\n",
        "\n",
        "# -----------------------\n",
        "# Helper functions\n",
        "# -----------------------\n",
        "def resize_frame(frame, target_w=TARGET_WIDTH):\n",
        "    \"\"\"Resize frame keeping aspect ratio.\"\"\"\n",
        "    h, w = frame.shape[:2]\n",
        "    if w <= target_w:\n",
        "        return frame, 1.0\n",
        "    scale = target_w / float(w)\n",
        "    new_h = int(h * scale)\n",
        "    resized = cv2.resize(frame, (target_w, new_h))\n",
        "    return resized, scale\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Main Detection & Tracking Loop\n",
        "# -----------------------\n",
        "def detect_and_track(source):\n",
        "    global running, recording, out, video_save_path\n",
        "\n",
        "    cap = cv2.VideoCapture(source)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Cannot open source.\")\n",
        "        return\n",
        "\n",
        "    frame_idx = 0\n",
        "    out = None\n",
        "\n",
        "    while running:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_idx += 1\n",
        "        if FRAME_SKIP > 1 and (frame_idx % FRAME_SKIP) != 0:\n",
        "            cv2.imshow(\"YOLOv8 + ByteTrack\", frame)\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "            continue\n",
        "\n",
        "        resized, scale = resize_frame(frame)\n",
        "\n",
        "        # YOLOv8 + ByteTrack tracking\n",
        "        results = model.track(\n",
        "            resized,\n",
        "            persist=True,\n",
        "            conf=CONF_THRESH,\n",
        "            tracker=\"bytetrack.yaml\",\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        # Draw detections\n",
        "        if len(results) > 0 and results[0].boxes is not None:\n",
        "            boxes = results[0].boxes.xyxy.cpu().numpy()\n",
        "            ids = results[0].boxes.id\n",
        "            cls = results[0].boxes.cls.cpu().numpy()\n",
        "\n",
        "            if ids is not None:\n",
        "                ids = ids.int().cpu().numpy()\n",
        "                for box, track_id, c in zip(boxes, ids, cls):\n",
        "                    if int(c) != PERSON_CLASS_ID:\n",
        "                        continue\n",
        "                    x1, y1, x2, y2 = box\n",
        "                    if scale != 1.0:\n",
        "                        inv = 1.0 / scale\n",
        "                        x1, y1, x2, y2 = [int(v * inv) for v in (x1, y1, x2, y2)]\n",
        "                    cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), GREEN, 2)\n",
        "                    cv2.putText(frame, f\"ID:{int(track_id)}\", (int(x1), int(y1) - 5),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, GREEN, 2)\n",
        "\n",
        "                current_count = len(set(ids))\n",
        "            else:\n",
        "                current_count = 0\n",
        "        else:\n",
        "            current_count = 0\n",
        "\n",
        "        # Display count\n",
        "        cv2.putText(frame, f\"People: {current_count}\", (20, 40),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, RED, 2)\n",
        "\n",
        "        # Recording logic\n",
        "        if recording:\n",
        "            if out is None:\n",
        "                filename = f\"yolo_track_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4\"\n",
        "                video_save_path = os.path.join(download_path, filename)\n",
        "                fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "                h, w = frame.shape[:2]\n",
        "                out = cv2.VideoWriter(video_save_path, fourcc, 20.0, (w, h))\n",
        "                print(\"üé• Recording ->\", video_save_path)\n",
        "            out.write(frame)\n",
        "\n",
        "        cv2.imshow(\"YOLOv8 + ByteTrack\", frame)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    if out is not None:\n",
        "        out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Tkinter UI Functions\n",
        "# -----------------------\n",
        "def start_tracking_video():\n",
        "    global running, using_webcam\n",
        "    if not video_path:\n",
        "        print(\"‚ö† Upload a video first.\")\n",
        "        return\n",
        "    if not running:\n",
        "        running = True\n",
        "        using_webcam = False\n",
        "        threading.Thread(target=detect_and_track, args=(video_path,), daemon=True).start()\n",
        "\n",
        "def start_tracking_webcam():\n",
        "    global running, using_webcam\n",
        "    if not running:\n",
        "        running = True\n",
        "        using_webcam = True\n",
        "        threading.Thread(target=detect_and_track, args=(0,), daemon=True).start()\n",
        "\n",
        "def stop_tracking():\n",
        "    global running\n",
        "    running = False\n",
        "\n",
        "def start_recording():\n",
        "    global recording, out\n",
        "    if not recording:\n",
        "        recording = True\n",
        "        out = None\n",
        "        print(\" Recording started\")\n",
        "\n",
        "def stop_recording():\n",
        "    global recording, out, video_save_path\n",
        "    if recording:\n",
        "        recording = False\n",
        "        if out is not None:\n",
        "            out.release()\n",
        "            out = None\n",
        "        if video_save_path:\n",
        "            print(\"Saved to:\", video_save_path)\n",
        "        else:\n",
        "            print(\"Recording stopped (no file)\")\n",
        "\n",
        "def upload_video():\n",
        "    global video_path\n",
        "    file_path = filedialog.askopenfilename(\n",
        "        title=\"Select a video file\",\n",
        "        filetypes=[(\"Video files\", \"*.mp4 *.avi *.mov *.mkv\")]\n",
        "    )\n",
        "    if file_path:\n",
        "        video_path = file_path\n",
        "        print(\" Video loaded:\", video_path)\n",
        "        video_label.config(text=f\"Video: {os.path.basename(video_path)}\", fg=\"lightgreen\")\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# GUI setup\n",
        "# -----------------------\n",
        "root = tk.Tk()\n",
        "root.title(\"YOLOv8 + ByteTrack Tracker\")\n",
        "root.geometry(\"460x480\")\n",
        "root.configure(bg=\"#202020\")\n",
        "\n",
        "style = ttk.Style()\n",
        "style.configure(\"TButton\", font=(\"Arial\", 12), padding=10)\n",
        "\n",
        "tk.Label(root, text=\"YOLOv8 + ByteTrack Tracker\", bg=\"#202020\", fg=\"white\",\n",
        "         font=(\"Arial\", 14)).pack(pady=15)\n",
        "\n",
        "ttk.Button(root, text=\" Start Webcam Tracking\", command=start_tracking_webcam).pack(pady=5)\n",
        "ttk.Button(root, text=\" Upload Video\", command=upload_video).pack(pady=5)\n",
        "video_label = tk.Label(root, text=\"No video selected\", bg=\"#202020\", fg=\"gray\", font=(\"Arial\", 10))\n",
        "video_label.pack()\n",
        "\n",
        "ttk.Button(root, text=\" Start Video Tracking\", command=start_tracking_video).pack(pady=5)\n",
        "ttk.Button(root, text=\" Stop Tracking\", command=stop_tracking).pack(pady=5)\n",
        "ttk.Button(root, text=\" Start Recording\", command=start_recording).pack(pady=5)\n",
        "ttk.Button(root, text=\" Stop Recording\", command=stop_recording).pack(pady=5)\n",
        "ttk.Button(root, text=\" Exit\", command=root.destroy).pack(pady=10)\n",
        "\n",
        "root.mainloop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd6a5667",
      "metadata": {
        "id": "bd6a5667"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# Faster R-CNN + DeepSORT + Tkinter Live Tracking (fixed counting + person-only)\n",
        "# ===============================\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import threading\n",
        "import tkinter as tk\n",
        "from tkinter import ttk\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "import os\n",
        "from datetime import datetime\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "\n",
        "# Load Faster R-CNN model\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Initialize DeepSORT\n",
        "tracker = DeepSort(max_age=30, n_init=3, nn_budget=70)\n",
        "\n",
        "# Global flags\n",
        "running = False\n",
        "recording = False\n",
        "out = None\n",
        "video_save_path = None\n",
        "\n",
        "# Output folder (Downloads)\n",
        "download_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
        "os.makedirs(download_path, exist_ok=True)\n",
        "\n",
        "# Colors\n",
        "GREEN = (0, 255, 0)\n",
        "RED = (0, 0, 255)\n",
        "\n",
        "# COCO person class id\n",
        "PERSON_CLASS_ID = 1\n",
        "\n",
        "def detect_and_track():\n",
        "    global running, recording, out, video_save_path\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Cannot open camera\")\n",
        "        return\n",
        "\n",
        "    # Use while running loop\n",
        "    while running:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Prepare image tensor\n",
        "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        img_tensor = torch.from_numpy(img / 255.).permute(2, 0, 1).float().unsqueeze(0)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            preds = model(img_tensor)[0]\n",
        "\n",
        "        boxes = preds['boxes'].cpu().numpy()          # (N,4) x1,y1,x2,y2\n",
        "        scores = preds['scores'].cpu().numpy()        # (N,)\n",
        "        labels = preds['labels'].cpu().numpy()        # (N,)\n",
        "\n",
        "        # Build detections list for DeepSORT: only person class\n",
        "        detections = []\n",
        "        CONF_THRESH = 0.5\n",
        "        for box, score, label in zip(boxes, scores, labels):\n",
        "            if score < CONF_THRESH:\n",
        "                continue\n",
        "            if int(label) != PERSON_CLASS_ID:\n",
        "                continue\n",
        "            x1, y1, x2, y2 = box\n",
        "            w = x2 - x1\n",
        "            h = y2 - y1\n",
        "            # DeepSort expects [x, y, w, h]\n",
        "            detections.append(([float(x1), float(y1), float(w), float(h)], float(score), \"person\"))\n",
        "\n",
        "        # Update/deepsort tracks\n",
        "        tracks = tracker.update_tracks(detections, frame=frame)\n",
        "\n",
        "        # Draw tracks and count confirmed active tracks\n",
        "        active_tracks = []\n",
        "        for track in tracks:\n",
        "            # DeepSort track API: is_confirmed(), time_since_update, to_ltrb(), track_id\n",
        "            if not track.is_confirmed():\n",
        "                continue\n",
        "            if track.time_since_update > 1:  # skip stale\n",
        "                continue\n",
        "            active_tracks.append(track)\n",
        "            x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
        "            track_id = track.track_id\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), GREEN, 2)\n",
        "            cv2.putText(frame, f\"ID: {track_id}\", (x1, max(y1 - 10, 10)),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, GREEN, 2)\n",
        "\n",
        "        # Current count = number of active confirmed tracks\n",
        "        current_count = len(active_tracks)\n",
        "        cv2.putText(frame, f\"Count: {current_count}\", (20, 40),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1.0, RED, 2)\n",
        "\n",
        "        # Initialize video writer if recording (use frame shape)\n",
        "        if recording:\n",
        "            if out is None:\n",
        "                # create unique filename\n",
        "                filename = f\"tracked_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4\"\n",
        "                video_save_path = os.path.join(download_path, filename)\n",
        "                fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "                h, w = frame.shape[:2]\n",
        "                out = cv2.VideoWriter(video_save_path, fourcc, 20.0, (w, h))\n",
        "                print(\"Recording ->\", video_save_path)\n",
        "            out.write(frame)\n",
        "        else:\n",
        "            # if not recording but writer exists, release it (safety)\n",
        "            if out is not None and video_save_path is None:\n",
        "                out.release()\n",
        "\n",
        "        cv2.imshow(\"Faster R-CNN + DeepSORT Tracking\", frame)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    if out is not None:\n",
        "        out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "def start_tracking():\n",
        "    global running\n",
        "    if not running:\n",
        "        running = True\n",
        "        threading.Thread(target=detect_and_track, daemon=True).start()\n",
        "\n",
        "def stop_tracking():\n",
        "    global running\n",
        "    running = False\n",
        "\n",
        "def start_recording():\n",
        "    global recording, out, video_save_path\n",
        "    if not recording:\n",
        "        recording = True\n",
        "        video_save_path = None\n",
        "        out = None\n",
        "        print(\"Recording started\")\n",
        "\n",
        "def stop_recording():\n",
        "    global recording, out, video_save_path\n",
        "    if recording:\n",
        "        recording = False\n",
        "        if out is not None:\n",
        "            out.release()\n",
        "            out = None\n",
        "        if video_save_path:\n",
        "            print(\"Recording saved to:\", video_save_path)\n",
        "        else:\n",
        "            print(\"Recording stopped (no file)\")\n",
        "\n",
        "# ===============================\n",
        "# Tkinter UI\n",
        "# ===============================\n",
        "root = tk.Tk()\n",
        "root.title(\"Faster R-CNN + DeepSORT Tracker\")\n",
        "root.geometry(\"400x300\")\n",
        "root.configure(bg=\"#202020\")\n",
        "\n",
        "style = ttk.Style()\n",
        "style.configure(\"TButton\", font=(\"Arial\", 12), padding=10)\n",
        "\n",
        "title_label = tk.Label(root, text=\" Faster R-CNN + DeepSORT Tracker\", bg=\"#202020\", fg=\"white\", font=(\"Arial\", 14))\n",
        "title_label.pack(pady=15)\n",
        "\n",
        "ttk.Button(root, text=\"Start Tracking\", command=start_tracking).pack(pady=5)\n",
        "ttk.Button(root, text=\"Stop Tracking\", command=stop_tracking).pack(pady=5)\n",
        "ttk.Button(root, text=\"Start Recording\", command=start_recording).pack(pady=5)\n",
        "ttk.Button(root, text=\"Stop Recording\", command=stop_recording).pack(pady=5)\n",
        "ttk.Button(root, text=\"Exit\", command=root.destroy).pack(pady=5)\n",
        "\n",
        "root.mainloop()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.10)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}